{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用PaddleNLP语义预训练大模型ERNIE与小模型PP-MiniLM优化情感分析\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 前置准备\n",
    "### 环境配置\n",
    "AI Studio平台默认安装了Paddle和PaddleNLP，并定期更新版本。 如需手动更新Paddle，可参考[飞桨安装说明](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/conda/linux-conda.html)，安装相应环境下最新版飞桨框架。\n",
    "\n",
    "使用如下命令确保安装最新版PaddleNLP："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.1.1)\n",
      "Collecting paddlenlp\n",
      "  Downloading paddlenlp-2.2.4-py3-none-any.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 26 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.19.5)\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Attempting uninstall: paddlenlp\n",
      "    Found existing installation: paddlenlp 2.1.1\n",
      "    Uninstalling paddlenlp-2.1.1:\n",
      "      Successfully uninstalled paddlenlp-2.1.1\n",
      "Successfully installed paddlenlp-2.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载数据集\n",
    "\n",
    "以公开中文情感分析数据集ChnSenticorp为例。PaddleNLP已经内置该数据集，一键即可加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/transformers/funnel/modeling.py:30: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "100%|██████████| 1909/1909 [00:00<00:00, 6525.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n",
      "{'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'label': 1, 'qid': ''}\n",
      "{'text': '15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错', 'label': 1, 'qid': ''}\n",
      "{'text': '房间太小。其他的都一般。。。。。。。。。', 'label': 0, 'qid': ''}\n",
      "{'text': '1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸，一摸一个印. 4.硬盘分区不好办.', 'label': 0, 'qid': ''}\n",
      "{'text': '今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量商量,单独出个第6卷,让我们的孩子不会有所遗憾。', 'label': 1, 'qid': ''}\n"
     ]
    }
   ],
   "source": [
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "train_ds, dev_ds, test_ds = load_dataset(\n",
    "    \"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "print(train_ds.label_list)\n",
    "\n",
    "for data in train_ds.data[:5]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "每条数据包含一句评论和对应的标签，0或1。0代表负向评论，1代表正向评论。\n",
    "\n",
    "之后，还需要对输入句子进行数据处理，如切词，映射词表id等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 一、使用大模型ERNIE完成中文情感分析任务\n",
    "###  调用`ppnlp.transformers.ErnieTokenizer`进行数据处理\n",
    "\n",
    "预训练模型ERNIE对中文数据的处理是以字为单位。PaddleNLP对于各种预训练模型已经内置了相应的tokenizer。指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer作用为将原始输入文本转化成模型model可以接受的输入数据形式。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "<br><center>图3：ERNIE模型框架示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-27 17:57:43,268] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie/vocab.txt and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
      "[2022-01-27 17:57:43,311] [    INFO] - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie/vocab.txt\n",
      "100%|██████████| 89.5k/89.5k [00:00<00:00, 4.25MB/s]\n",
      "[2022-01-27 17:57:43,421] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
      "[2022-01-27 17:57:43,424] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie/ernie_v1_chn_base.pdparams\n",
      "100%|██████████| 383M/383M [00:11<00:00, 35.9MB/s] \n",
      "W0127 17:57:54.729671   101 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0127 17:57:54.734345   101 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n",
      "[2022-01-27 17:58:00,850] [    INFO] - Weights from pretrained model not used in ErnieModel: ['cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.bias']\n"
     ]
    }
   ],
   "source": [
    "# 设置想要使用模型的名称\n",
    "MODEL_NAME = \"ernie-1.0\"\n",
    "\n",
    "tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)\n",
    "ernie_model = ppnlp.transformers.ErnieModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['请', '输', '入', '测', '试', '样', '例']\n",
      "Tokens id: [647, 789, 109, 558, 525, 314, 656]\n",
      "Tokens : Tensor(shape=[1, 9], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[1  , 647, 789, 109, 558, 525, 314, 656, 2  ]])\n",
      "Token wise output: [1, 9, 768], Pooled output: [1, 768]\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "# 将原始输入文本切分token，\n",
    "tokens = tokenizer._tokenize(\"请输入测试样例\")\n",
    "print(\"Tokens: {}\".format(tokens))\n",
    "\n",
    "# token映射为对应token id\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens id: {}\".format(tokens_ids))\n",
    "\n",
    "\n",
    "# 拼接上预训练模型对应的特殊token ，如[CLS]、[SEP]\n",
    "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "\n",
    "# 转化成paddle框架数据格式\n",
    "tokens_pd = paddle.to_tensor([tokens_ids])\n",
    "print(\"Tokens : {}\".format(tokens_pd))\n",
    "\n",
    "# 此时即可输入ERNIE模型中得到相应输出\n",
    "sequence_output, pooled_output = ernie_model(tokens_pd)\n",
    "print(\"Token wise output: {}, Pooled output: {}\".format(sequence_output.shape, pooled_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从以上代码可以看出，ERNIE模型输出有2个tensor。\n",
    "\n",
    "* `sequence_output`是对应每个输入token的语义特征表示，shape为(1, num_tokens, hidden_size)。其一般用于序列标注、问答等任务。\n",
    "* `pooled_output`是对应整个句子的语义特征表示，shape为(1, hidden_size)。其一般用于文本分类、信息检索等任务。\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "如需使用ernie-tiny预训练模型，则对应的tokenizer应该使用`paddlenlp.transformers.ErnieTinyTokenizer.from_pretrained('ernie-tiny')`\n",
    "\n",
    "以上代码示例展示了使用Transformer类预训练模型所需的数据处理步骤。为了更方便地使用，PaddleNLP同时提供了更加高阶API，一键即可返回模型所需数据格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\t[1, 647, 789, 109, 558, 525, 314, 656, 2]\n",
      "token_type_ids:\n",
      "\t[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "input_ids : Tensor(shape=[1, 9], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[1  , 647, 789, 109, 558, 525, 314, 656, 2  ]])\n",
      "token_type_ids : Tensor(shape=[1, 9], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Token wise output: [1, 9, 768], Pooled output: [1, 768]\n"
     ]
    }
   ],
   "source": [
    "# 一行代码完成切分token，映射token ID以及拼接特殊token\n",
    "encoded_text = tokenizer(text=\"请输入测试样例\")\n",
    "for key, value in encoded_text.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))\n",
    "\n",
    "# 转化成paddle框架数据格式\n",
    "input_ids = paddle.to_tensor([encoded_text['input_ids']])\n",
    "print(\"input_ids : {}\".format(input_ids))\n",
    "segment_ids = paddle.to_tensor([encoded_text['token_type_ids']])\n",
    "print(\"token_type_ids : {}\".format(segment_ids))\n",
    "\n",
    "# 此时即可输入ERNIE模型中得到相应输出\n",
    "sequence_output, pooled_output = ernie_model(input_ids, segment_ids)\n",
    "print(\"Token wise output: {}, Pooled output: {}\".format(sequence_output.shape, pooled_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "由以上代码可以见，tokenizer提供了一种非常便利的方式生成模型所需的数据格式。\n",
    "\n",
    "以上，\n",
    "* `input_ids`: 表示输入文本的token ID。\n",
    "* `segment_ids`: 表示对应的token属于输入的第一个句子还是第二个句子。（Transformer类预训练模型支持单句以及句对输入。）详细参见左侧utils.py文件中`convert_example()`函数解释。\n",
    "* `seq_len`： 表示输入句子的token个数。\n",
    "* `input_mask`：表示对应的token是否一个padding token。由于一个batch中的输入句子长度不同，所以需要将不同长度的句子padding到统一固定长度。1表示真实输入，0表示对应token为padding token。\n",
    "* `position_ids`: 表示对应token在整个输入序列中的位置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单句输入token (str): ['[CLS]', '请', '输', '入', '测', '试', '样', '例', '[SEP]']\n",
      "单句输入token (int): [1, 647, 789, 109, 558, 525, 314, 656, 2]\n",
      "单句输入segment ids : [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "句对输入token (str): ['[CLS]', '请', '输', '入', '测', '试', '样', '例', '1', '[SEP]', '请', '输', '入', '测', '试', '样', '例', '2', '[SEP]']\n",
      "句对输入token (int): [1, 647, 789, 109, 558, 525, 314, 656, 208, 2, 647, 789, 109, 558, 525, 314, 656, 249, 2]\n",
      "句对输入segment ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 单句输入\n",
    "single_seg_input = tokenizer(text=\"请输入测试样例\")\n",
    "# 句对输入\n",
    "multi_seg_input = tokenizer(text=\"请输入测试样例1\", text_pair=\"请输入测试样例2\")\n",
    "\n",
    "print(\"单句输入token (str): {}\".format(tokenizer.convert_ids_to_tokens(single_seg_input['input_ids'])))\n",
    "print(\"单句输入token (int): {}\".format(single_seg_input['input_ids']))\n",
    "print(\"单句输入segment ids : {}\".format(single_seg_input['token_type_ids']))\n",
    "\n",
    "print()\n",
    "print(\"句对输入token (str): {}\".format(tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids'])))\n",
    "print(\"句对输入token (int): {}\".format(multi_seg_input['input_ids']))\n",
    "print(\"句对输入segment ids : {}\".format(multi_seg_input['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\t[1, 647, 789, 109, 558, 525, 314, 656, 2]\n",
      "token_type_ids:\n",
      "\t[0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Highlight: padding到统一长度\n",
    "encoded_text = tokenizer(text=\"请输入测试样例\",  max_seq_len=15)\n",
    "\n",
    "for key, value in encoded_text.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**以上代码示例详细介绍了tokenizer的用法。**\n",
    "\n",
    "**接下来使用tokenzier处理ChnSentiCorp数据集。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据读入\n",
    "\n",
    "使用`paddle.io.DataLoader`接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from utils import  convert_example, create_dataloader\n",
    "\n",
    "# 模型运行批处理大小\n",
    "batch_size = 32\n",
    "max_seq_length = 128\n",
    "\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PaddleNLP一键加载预训练模型\n",
    "\n",
    "\n",
    "情感分析本质是一个文本分类任务，PaddleNLP对于各种预训练模型已经内置了对于下游任务-文本分类的Fine-tune网络。以下教程ERNIE为例，介绍如何将预训练模型Fine-tune完成文本分类任务。\n",
    "\n",
    "### `paddlenlp.transformers.ErnieModel()`一行代码即可加载预训练模型ERNIE。\n",
    "\n",
    "### `paddlenlp.transformers.ErnieForSequenceClassification()`一行代码即可加载预训练模型ERNIE用于文本分类任务的Fine-tune网络。\n",
    "其在ERNIE模型后拼接上一个全连接网络（Full Connected）进行分类。\n",
    "\n",
    "### `paddlenlp.transformers.ErnieForSequenceClassification.from_pretrained()` 只需指定想要使用的模型名称和文本分类的类别数即可完成网络定义。\n",
    "\n",
    "PaddleNLP不仅支持ERNIE预训练模型，还支持BERT、RoBERTa、Electra等预训练模型，可跳转到文末了解更多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-27 17:58:49,685] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2022-01-27 17:58:50,782] [    INFO] - Weights from pretrained model not used in ErnieModel: ['cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.bias']\n",
      "[2022-01-27 17:58:51,113] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n"
     ]
    }
   ],
   "source": [
    "ernie_model = ppnlp.transformers.ErnieModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=len(train_ds.label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 设置Fine-Tune优化策略，接入评价指标\n",
    "适用于ERNIE/BERT这类Transformer模型的学习率为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p>\n",
    "<br><center>图4：动态学习率示意图</center></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 训练过程中的最大学习率\n",
    "learning_rate = 5e-5 \n",
    "# 训练轮次\n",
    "epochs = 1 #3\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练与评估\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。将前向计算结果传给评价方法，计算评价指标。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序将会评估一次，评估当前模型训练的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/checkpoint’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# checkpoint文件夹用于保存训练模型\n",
    "!mkdir /home/aistudio/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.67625, acc: 0.55625\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.69945, acc: 0.59531\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.33008, acc: 0.67604\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.30141, acc: 0.72891\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.21657, acc: 0.76062\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.29679, acc: 0.78385\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.55246, acc: 0.79955\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.39794, acc: 0.80742\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.27489, acc: 0.81701\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.14556, acc: 0.82563\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.25819, acc: 0.83153\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.20711, acc: 0.83646\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.15510, acc: 0.84231\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.09495, acc: 0.84754\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.11853, acc: 0.85292\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.18202, acc: 0.85703\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.10911, acc: 0.85993\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.03727, acc: 0.86250\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.26329, acc: 0.86398\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.38955, acc: 0.86484\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.29458, acc: 0.86815\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.27821, acc: 0.87045\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.27265, acc: 0.87337\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.29306, acc: 0.87526\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.41561, acc: 0.87638\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.24571, acc: 0.87873\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.12707, acc: 0.88125\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.15423, acc: 0.88248\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.15756, acc: 0.88427\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.09940, acc: 0.88594\n",
      "eval loss: 0.19790, accu: 0.92083\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "from utils import evaluate\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    evaluate(model, criterion, metric, dev_data_loader)\n",
    "\n",
    "model.save_pretrained('/home/aistudio/checkpoint')\n",
    "tokenizer.save_pretrained('/home/aistudio/checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型预测\n",
    "\n",
    "训练保存好的训练，即可用于预测。如以下示例代码自定义预测数据，调用`predict()`函数即可一键预测。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {'text': '这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般'} \t Lable: negative\n",
      "Data: {'text': '怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片'} \t Lable: negative\n",
      "Data: {'text': '作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。'} \t Lable: positive\n"
     ]
    }
   ],
   "source": [
    "from utils import predict\n",
    "\n",
    "data = [\n",
    "    {\"text\":'这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般'},\n",
    "    {\"text\":'怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片'},\n",
    "    {\"text\":'作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。'},\n",
    "]\n",
    "label_map = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "results = predict(\n",
    "    model, data, tokenizer, label_map, batch_size=batch_size)\n",
    "for idx, text in enumerate(data):\n",
    "    print('Data: {} \\t Lable: {}'.format(text, results[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、使用小模型PP-MiniLM完成中文情感分析任务\n",
    "\n",
    "PaddleNLP不仅支持ERNIE预训练模型，还支持BERT、RoBERTa、Electra等预训练模型。\n",
    "下表汇总了目前PaddleNLP支持的各类预训练模型。用户可以使用PaddleNLP提供的模型，完成问答、序列分类、token分类等任务。同时我们提供了22种预训练的参数权重供用户使用，其中包含了11种中文语言模型的预训练权重。\n",
    "\n",
    "| Model | Tokenizer| Supported Task| Model Name|\n",
    "|---|---|---|---|\n",
    "|[ERNIE](https://arxiv.org/abs/1904.09223)|ErnieTokenizer<br>ErnieTinyTokenizer|ErnieModel<br> ErnieForQuestionAnswering<br> ErnieForSequenceClassification<br> ErnieForTokenClassification| `ernie-1.0`<br> `ernie-tiny`<br> `ernie-2.0-en`<br> `ernie-2.0-large-en`|\n",
    "| [BERT](https://arxiv.org/abs/1810.04805) | BertTokenizer|BertModel<br> BertForQuestionAnswering<br> BertForSequenceClassification<br>BertForTokenClassification| `bert-base-uncased`<br> `bert-large-uncased` <br>`bert-base-multilingual-uncased` <br>`bert-base-cased`<br> `bert-base-chinese`<br> `bert-base-multilingual-cased`<br> `bert-large-cased`<br> `bert-wwm-chinese`<br> `bert-wwm-ext-chinese` |\n",
    "|[RoBERTa](https://arxiv.org/abs/1907.11692)|RobertaTokenizer| RobertaModel<br>RobertaForQuestionAnswering<br>RobertaForSequenceClassification<br>RobertaForTokenClassification| `roberta-wwm-ext`<br> `roberta-wwm-ext-large`<br> `rbt3`<br> `rbtl3`|\n",
    "|[ELECTRA](https://arxiv.org/abs/2003.10555) |ElectraTokenizer| ElectraModel<br>ElectraForSequenceClassification<br>ElectraForTokenClassification<br>|`electra-small`<br> `electra-base`<br> `electra-large`<br> `chinese-electra-small`<br> `chinese-electra-base`<br>|\n",
    "\n",
    "注：其中中文的预训练模型有 `bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small` 等。\n",
    "\n",
    "\n",
    "\n",
    "更多预训练模型参考：https://github.com/PaddlePaddle/models/blob/develop/PaddleNLP/docs/transformers.md\n",
    "更多预训练模型fine-tune下游任务使用方法，请参考[examples](https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/examples)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-27 18:26:14,507] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ppminilm-6l-768h/vocab.txt\n",
      "[2022-01-27 18:26:14,525] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ppminilm-6l-768h/ppminilm-6l-768h.pdparams\n",
      "[2022-01-27 18:26:14,528] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ppminilm-6l-768h/vocab.txt\n",
      "[2022-01-27 18:26:15,444] [    INFO] - Loaded parameters from /home/aistudio/.paddlenlp/models/ppminilm-6l-768h/ppminilm-6l-768h.pdparams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/checkpoint_miniLM’: File exists\n",
      "global step 10, epoch: 1, batch: 10, loss: 0.68872, acc: 0.53438\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.72103, acc: 0.53750\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.67698, acc: 0.52500\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.67032, acc: 0.52812\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.69583, acc: 0.52563\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.67224, acc: 0.52969\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.65289, acc: 0.53795\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.73905, acc: 0.53945\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.79083, acc: 0.53854\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.66158, acc: 0.54094\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.65248, acc: 0.54460\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.79817, acc: 0.54349\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.58708, acc: 0.54880\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.68914, acc: 0.54911\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.61882, acc: 0.54812\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.58035, acc: 0.55176\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.63940, acc: 0.55496\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.72506, acc: 0.55469\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.70032, acc: 0.55510\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.86756, acc: 0.55469\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.70730, acc: 0.55759\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.66564, acc: 0.56065\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.71124, acc: 0.56413\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.61586, acc: 0.56862\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.53684, acc: 0.57288\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.54985, acc: 0.57692\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.39817, acc: 0.58137\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.65867, acc: 0.58538\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.49370, acc: 0.59095\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.56592, acc: 0.59365\n",
      "eval loss: 0.56252, accu: 0.71917\n",
      "global step 310, epoch: 2, batch: 10, loss: 0.65453, acc: 0.70312\n",
      "global step 320, epoch: 2, batch: 20, loss: 0.50692, acc: 0.70625\n",
      "global step 330, epoch: 2, batch: 30, loss: 0.49054, acc: 0.72604\n",
      "global step 340, epoch: 2, batch: 40, loss: 0.46075, acc: 0.72734\n",
      "global step 350, epoch: 2, batch: 50, loss: 0.49762, acc: 0.72562\n",
      "global step 360, epoch: 2, batch: 60, loss: 0.47797, acc: 0.73646\n",
      "global step 370, epoch: 2, batch: 70, loss: 0.42267, acc: 0.74911\n",
      "global step 380, epoch: 2, batch: 80, loss: 0.55156, acc: 0.75117\n",
      "global step 390, epoch: 2, batch: 90, loss: 0.56358, acc: 0.74722\n",
      "global step 400, epoch: 2, batch: 100, loss: 0.52003, acc: 0.74750\n",
      "global step 410, epoch: 2, batch: 110, loss: 0.33481, acc: 0.75000\n",
      "global step 420, epoch: 2, batch: 120, loss: 0.51683, acc: 0.75443\n",
      "global step 430, epoch: 2, batch: 130, loss: 0.47313, acc: 0.75721\n",
      "global step 440, epoch: 2, batch: 140, loss: 0.53929, acc: 0.76183\n",
      "global step 450, epoch: 2, batch: 150, loss: 0.47582, acc: 0.76292\n",
      "global step 460, epoch: 2, batch: 160, loss: 0.59241, acc: 0.76309\n",
      "global step 470, epoch: 2, batch: 170, loss: 0.35624, acc: 0.76305\n",
      "global step 480, epoch: 2, batch: 180, loss: 0.36630, acc: 0.76528\n",
      "global step 490, epoch: 2, batch: 190, loss: 0.40341, acc: 0.76957\n",
      "global step 500, epoch: 2, batch: 200, loss: 0.43037, acc: 0.77016\n",
      "global step 510, epoch: 2, batch: 210, loss: 0.47606, acc: 0.76890\n",
      "global step 520, epoch: 2, batch: 220, loss: 0.31299, acc: 0.76875\n",
      "global step 530, epoch: 2, batch: 230, loss: 0.49745, acc: 0.76916\n",
      "global step 540, epoch: 2, batch: 240, loss: 0.38385, acc: 0.77018\n",
      "global step 550, epoch: 2, batch: 250, loss: 0.24177, acc: 0.77338\n",
      "global step 560, epoch: 2, batch: 260, loss: 0.52698, acc: 0.77404\n",
      "global step 570, epoch: 2, batch: 270, loss: 0.43775, acc: 0.77569\n",
      "global step 580, epoch: 2, batch: 280, loss: 0.31770, acc: 0.77768\n",
      "global step 590, epoch: 2, batch: 290, loss: 0.42675, acc: 0.77845\n",
      "global step 600, epoch: 2, batch: 300, loss: 0.41163, acc: 0.78031\n",
      "eval loss: 0.38939, accu: 0.84250\n",
      "global step 610, epoch: 3, batch: 10, loss: 0.42388, acc: 0.82188\n",
      "global step 620, epoch: 3, batch: 20, loss: 0.33448, acc: 0.81406\n",
      "global step 630, epoch: 3, batch: 30, loss: 0.47283, acc: 0.82292\n",
      "global step 640, epoch: 3, batch: 40, loss: 0.20215, acc: 0.82891\n",
      "global step 650, epoch: 3, batch: 50, loss: 0.40333, acc: 0.83250\n",
      "global step 660, epoch: 3, batch: 60, loss: 0.29096, acc: 0.83333\n",
      "global step 670, epoch: 3, batch: 70, loss: 0.25898, acc: 0.84018\n",
      "global step 680, epoch: 3, batch: 80, loss: 0.61094, acc: 0.83828\n",
      "global step 690, epoch: 3, batch: 90, loss: 0.31277, acc: 0.84132\n",
      "global step 700, epoch: 3, batch: 100, loss: 0.28077, acc: 0.84125\n",
      "global step 710, epoch: 3, batch: 110, loss: 0.39922, acc: 0.83920\n",
      "global step 720, epoch: 3, batch: 120, loss: 0.32228, acc: 0.84010\n",
      "global step 730, epoch: 3, batch: 130, loss: 0.46707, acc: 0.84111\n",
      "global step 740, epoch: 3, batch: 140, loss: 0.45476, acc: 0.84241\n",
      "global step 750, epoch: 3, batch: 150, loss: 0.42564, acc: 0.84292\n",
      "global step 760, epoch: 3, batch: 160, loss: 0.33959, acc: 0.84180\n",
      "global step 770, epoch: 3, batch: 170, loss: 0.35762, acc: 0.84081\n",
      "global step 780, epoch: 3, batch: 180, loss: 0.42345, acc: 0.84149\n",
      "global step 790, epoch: 3, batch: 190, loss: 0.40414, acc: 0.84145\n",
      "global step 800, epoch: 3, batch: 200, loss: 0.37491, acc: 0.84313\n",
      "global step 810, epoch: 3, batch: 210, loss: 0.32928, acc: 0.84435\n",
      "global step 820, epoch: 3, batch: 220, loss: 0.39534, acc: 0.84545\n",
      "global step 830, epoch: 3, batch: 230, loss: 0.26723, acc: 0.84674\n",
      "global step 840, epoch: 3, batch: 240, loss: 0.38607, acc: 0.84870\n",
      "global step 850, epoch: 3, batch: 250, loss: 0.43473, acc: 0.84950\n",
      "global step 860, epoch: 3, batch: 260, loss: 0.28739, acc: 0.84988\n",
      "global step 870, epoch: 3, batch: 270, loss: 0.30379, acc: 0.84977\n",
      "global step 880, epoch: 3, batch: 280, loss: 0.22894, acc: 0.84978\n",
      "global step 890, epoch: 3, batch: 290, loss: 0.47885, acc: 0.85129\n",
      "global step 900, epoch: 3, batch: 300, loss: 0.23490, acc: 0.85281\n",
      "eval loss: 0.39048, accu: 0.84667\n",
      "global step 910, epoch: 4, batch: 10, loss: 0.30104, acc: 0.89375\n",
      "global step 920, epoch: 4, batch: 20, loss: 0.29956, acc: 0.90000\n",
      "global step 930, epoch: 4, batch: 30, loss: 0.29757, acc: 0.89479\n",
      "global step 940, epoch: 4, batch: 40, loss: 0.23370, acc: 0.89531\n",
      "global step 950, epoch: 4, batch: 50, loss: 0.24794, acc: 0.88813\n",
      "global step 960, epoch: 4, batch: 60, loss: 0.13324, acc: 0.88906\n",
      "global step 970, epoch: 4, batch: 70, loss: 0.39414, acc: 0.88750\n",
      "global step 980, epoch: 4, batch: 80, loss: 0.11865, acc: 0.88828\n",
      "global step 990, epoch: 4, batch: 90, loss: 0.11208, acc: 0.88854\n",
      "global step 1000, epoch: 4, batch: 100, loss: 0.15102, acc: 0.88969\n",
      "global step 1010, epoch: 4, batch: 110, loss: 0.42404, acc: 0.88920\n",
      "global step 1020, epoch: 4, batch: 120, loss: 0.25527, acc: 0.89062\n",
      "global step 1030, epoch: 4, batch: 130, loss: 0.30694, acc: 0.88990\n",
      "global step 1040, epoch: 4, batch: 140, loss: 0.18717, acc: 0.89129\n",
      "global step 1050, epoch: 4, batch: 150, loss: 0.29434, acc: 0.89062\n",
      "global step 1060, epoch: 4, batch: 160, loss: 0.33384, acc: 0.89082\n",
      "global step 1070, epoch: 4, batch: 170, loss: 0.38951, acc: 0.88989\n",
      "global step 1080, epoch: 4, batch: 180, loss: 0.36436, acc: 0.89010\n",
      "global step 1090, epoch: 4, batch: 190, loss: 0.19355, acc: 0.89062\n",
      "global step 1100, epoch: 4, batch: 200, loss: 0.26183, acc: 0.89031\n",
      "global step 1110, epoch: 4, batch: 210, loss: 0.50748, acc: 0.88795\n",
      "global step 1120, epoch: 4, batch: 220, loss: 0.15980, acc: 0.88807\n",
      "global step 1130, epoch: 4, batch: 230, loss: 0.12038, acc: 0.88845\n",
      "global step 1140, epoch: 4, batch: 240, loss: 0.12635, acc: 0.88906\n",
      "global step 1150, epoch: 4, batch: 250, loss: 0.29949, acc: 0.88913\n",
      "global step 1160, epoch: 4, batch: 260, loss: 0.39236, acc: 0.89014\n",
      "global step 1170, epoch: 4, batch: 270, loss: 0.35430, acc: 0.89005\n",
      "global step 1180, epoch: 4, batch: 280, loss: 0.24187, acc: 0.89018\n",
      "global step 1190, epoch: 4, batch: 290, loss: 0.17051, acc: 0.89127\n",
      "global step 1200, epoch: 4, batch: 300, loss: 0.53039, acc: 0.89167\n",
      "eval loss: 0.39557, accu: 0.86250\n",
      "global step 1210, epoch: 5, batch: 10, loss: 0.33104, acc: 0.90938\n",
      "global step 1220, epoch: 5, batch: 20, loss: 0.15967, acc: 0.91406\n",
      "global step 1230, epoch: 5, batch: 30, loss: 0.12689, acc: 0.91875\n",
      "global step 1240, epoch: 5, batch: 40, loss: 0.15162, acc: 0.92031\n",
      "global step 1250, epoch: 5, batch: 50, loss: 0.45100, acc: 0.92375\n",
      "global step 1260, epoch: 5, batch: 60, loss: 0.21745, acc: 0.92083\n",
      "global step 1270, epoch: 5, batch: 70, loss: 0.14816, acc: 0.92054\n",
      "global step 1280, epoch: 5, batch: 80, loss: 0.09223, acc: 0.92188\n",
      "global step 1290, epoch: 5, batch: 90, loss: 0.20753, acc: 0.91910\n",
      "global step 1300, epoch: 5, batch: 100, loss: 0.20360, acc: 0.91812\n",
      "global step 1310, epoch: 5, batch: 110, loss: 0.16939, acc: 0.91989\n",
      "global step 1320, epoch: 5, batch: 120, loss: 0.08311, acc: 0.92109\n",
      "global step 1330, epoch: 5, batch: 130, loss: 0.40647, acc: 0.92043\n",
      "global step 1340, epoch: 5, batch: 140, loss: 0.50030, acc: 0.91830\n",
      "global step 1350, epoch: 5, batch: 150, loss: 0.34366, acc: 0.91667\n",
      "global step 1360, epoch: 5, batch: 160, loss: 0.23160, acc: 0.91523\n",
      "global step 1370, epoch: 5, batch: 170, loss: 0.39268, acc: 0.91434\n",
      "global step 1380, epoch: 5, batch: 180, loss: 0.28341, acc: 0.91181\n",
      "global step 1390, epoch: 5, batch: 190, loss: 0.19098, acc: 0.91151\n",
      "global step 1400, epoch: 5, batch: 200, loss: 0.16044, acc: 0.91125\n",
      "global step 1410, epoch: 5, batch: 210, loss: 0.20216, acc: 0.91101\n",
      "global step 1420, epoch: 5, batch: 220, loss: 0.28080, acc: 0.91278\n",
      "global step 1430, epoch: 5, batch: 230, loss: 0.08285, acc: 0.91332\n",
      "global step 1440, epoch: 5, batch: 240, loss: 0.12990, acc: 0.91406\n",
      "global step 1450, epoch: 5, batch: 250, loss: 0.23069, acc: 0.91387\n",
      "global step 1460, epoch: 5, batch: 260, loss: 0.30178, acc: 0.91514\n",
      "global step 1470, epoch: 5, batch: 270, loss: 0.06390, acc: 0.91644\n",
      "global step 1480, epoch: 5, batch: 280, loss: 0.22835, acc: 0.91685\n",
      "global step 1490, epoch: 5, batch: 290, loss: 0.13713, acc: 0.91627\n",
      "global step 1500, epoch: 5, batch: 300, loss: 0.18805, acc: 0.91635\n",
      "eval loss: 0.39367, accu: 0.85833\n",
      "global step 1510, epoch: 6, batch: 10, loss: 0.28768, acc: 0.94688\n",
      "global step 1520, epoch: 6, batch: 20, loss: 0.15590, acc: 0.95000\n",
      "global step 1530, epoch: 6, batch: 30, loss: 0.18326, acc: 0.94792\n",
      "global step 1540, epoch: 6, batch: 40, loss: 0.07773, acc: 0.94531\n",
      "global step 1550, epoch: 6, batch: 50, loss: 0.14195, acc: 0.94563\n",
      "global step 1560, epoch: 6, batch: 60, loss: 0.20821, acc: 0.94427\n",
      "global step 1570, epoch: 6, batch: 70, loss: 0.23349, acc: 0.94196\n",
      "global step 1580, epoch: 6, batch: 80, loss: 0.17167, acc: 0.94375\n",
      "global step 1590, epoch: 6, batch: 90, loss: 0.17713, acc: 0.94340\n",
      "global step 1600, epoch: 6, batch: 100, loss: 0.11573, acc: 0.94500\n",
      "global step 1610, epoch: 6, batch: 110, loss: 0.19973, acc: 0.94602\n",
      "global step 1620, epoch: 6, batch: 120, loss: 0.18945, acc: 0.94583\n",
      "global step 1630, epoch: 6, batch: 130, loss: 0.14535, acc: 0.94495\n",
      "global step 1640, epoch: 6, batch: 140, loss: 0.38742, acc: 0.94420\n",
      "global step 1650, epoch: 6, batch: 150, loss: 0.13852, acc: 0.94354\n",
      "global step 1660, epoch: 6, batch: 160, loss: 0.22115, acc: 0.94336\n",
      "global step 1670, epoch: 6, batch: 170, loss: 0.08669, acc: 0.94375\n",
      "global step 1680, epoch: 6, batch: 180, loss: 0.09881, acc: 0.94340\n",
      "global step 1690, epoch: 6, batch: 190, loss: 0.04262, acc: 0.94293\n",
      "global step 1700, epoch: 6, batch: 200, loss: 0.26268, acc: 0.94391\n",
      "global step 1710, epoch: 6, batch: 210, loss: 0.12557, acc: 0.94301\n",
      "global step 1720, epoch: 6, batch: 220, loss: 0.08646, acc: 0.94276\n",
      "global step 1730, epoch: 6, batch: 230, loss: 0.28848, acc: 0.94198\n",
      "global step 1740, epoch: 6, batch: 240, loss: 0.36793, acc: 0.94154\n",
      "global step 1750, epoch: 6, batch: 250, loss: 0.10324, acc: 0.94175\n",
      "global step 1760, epoch: 6, batch: 260, loss: 0.16434, acc: 0.94171\n",
      "global step 1770, epoch: 6, batch: 270, loss: 0.33770, acc: 0.94167\n",
      "global step 1780, epoch: 6, batch: 280, loss: 0.06832, acc: 0.94185\n",
      "global step 1790, epoch: 6, batch: 290, loss: 0.04743, acc: 0.94159\n",
      "global step 1800, epoch: 6, batch: 300, loss: 0.30306, acc: 0.94115\n",
      "eval loss: 0.41327, accu: 0.85833\n"
     ]
    }
   ],
   "source": [
    "# 设置想要使用模型的名称\n",
    "MODEL_NAME = \"ppminilm-6l-768h\"\n",
    "\n",
    "tokenizer = ppnlp.transformers.PPMiniLMTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 一键加载预训练模型，将ernie更换为PP-miniLM \n",
    "model = ppnlp.transformers.PPMiniLMForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=len(train_ds.label_list))\n",
    "\n",
    "# 设置Fine-Tune优化策略，接入评价指标\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 训练过程中的最大学习率\n",
    "learning_rate = 5e-5 \n",
    "# 训练轮次\n",
    "epochs = 6 #3\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "# checkpoint文件夹用于保存训练模型\n",
    "!mkdir /home/aistudio/checkpoint_miniLM\n",
    "\n",
    "# 模型训练与评估\n",
    "import paddle.nn.functional as F\n",
    "from utils import evaluate\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    evaluate(model, criterion, metric, dev_data_loader)\n",
    "\n",
    "model.save_pretrained('/home/aistudio/checkpoint')\n",
    "tokenizer.save_pretrained('/home/aistudio/checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {'text': '这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般'} \t Lable: negative\n",
      "Data: {'text': '怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片'} \t Lable: negative\n",
      "Data: {'text': '作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。'} \t Lable: positive\n"
     ]
    }
   ],
   "source": [
    "# 模型预测\n",
    "from utils import predict\n",
    "\n",
    "data = [\n",
    "    {\"text\":'这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般'},\n",
    "    {\"text\":'怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片'},\n",
    "    {\"text\":'作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。'},\n",
    "]\n",
    "label_map = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "results = predict(\n",
    "    model, data, tokenizer, label_map, batch_size=batch_size)\n",
    "for idx, text in enumerate(data):\n",
    "    print('Data: {} \\t Lable: {}'.format(text, results[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、大模型ERNIE与小模型PP-miniLM对比\n",
    "\n",
    "- 大模型ERNIE对中文语境下的nlp任务表现出非常强的能力，仅一轮Fine-Tune就实现了目标数据集上90%的准确性。\n",
    "- 小模型PP-miniLM优势在于体积小速度快，相应的Fine-Tune可能需要多增加几个轮次，但最终也可以取得86%的准确性，表现非常不错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PaddleNLP更多教程\n",
    "\n",
    "- [使用seq2vec模块进行句子情感分类](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    "- [使用BiGRU-CRF模型完成快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)\n",
    "- [使用预训练模型ERNIE优化快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    "- [使用Seq2Seq模型完成自动对联](https://aistudio.baidu.com/aistudio/projectdetail/1321118)\n",
    "- [使用预训练模型ERNIE-GEN实现智能写诗](https://aistudio.baidu.com/aistudio/projectdetail/1339888)\n",
    "- [使用TCN网络完成新冠疫情病例数预测](https://aistudio.baidu.com/aistudio/projectdetail/1290873)\n",
    "- [使用预训练模型完成阅读理解](https://aistudio.baidu.com/aistudio/projectdetail/1339612)\n",
    "- [自定义数据集实现文本多分类任务](https://aistudio.baidu.com/aistudio/projectdetail/1468469)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
